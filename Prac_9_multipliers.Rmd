---
title: Exercise 9<br> Analysis with multipliers
author: Centre for Research into Ecological and Environmental Modelling <br> **University of St Andrews**
date: Introduction to distance sampling<br> April 2022
output: 
  rmdformats::readthedown:
    highlight: tango
bibliography: references.bib
csl: apa.csl
---

```{r, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We consider indirect methods to estimate abundance and hence include multipliers in the abundance calculations. The first problem uses data from a dung survey of deer and there are two levels of multipliers that need to be accounted for (dung production rate and dung decay rate). Data sets [2](#cue-counting-survey-of-whales) and [3](#cue-counting-of-songbirds-optional) deal with instantaneous cues and so only cue rate needs to be taken into account.

# Objectives

The objectives of this exercise are to

1.  Fit detection functions to cues
2.  Obtain relevant multipliers
3.  Use the multipliers in the `dht2` function to obtain animal abundances.

# Dung survey of deer

The question is how to estimate of the density of sika deer in a number of woodlands in the Scottish Borders [@marques2001]. These animals are shy and will be aware of the presence of an observer before the observer detects them, making surveys of this species challenging. As a consequence, indirect estimation methods have been applied to this problem. In this manner, an estimate of density is produced for some sign generated by deer (in this case, faecal or dung pellets) and this estimate is transformed to density of deer ($D_{\textrm{deer}}$) by

$$ \hat D_{\textrm{deer}} = \frac{\textrm{dung deposited daily}}{\textrm{dung production rate (per animal)}} $$ where the dung deposited daily is given by

$$ \textrm{dung deposited daily} = \frac{\hat D_{\textrm{pellet groups}}}{\textrm{mean time to decay}} $$ Hence, we use distance sampling to produce a pellet group density estimate, then adjust it accordingly to account for the production and decay processes operating during the time the data were being acquired. We will also take uncertainty in the dung production and decay rates into account in our final estimate of deer density.

Data from 9 woodlands (labelled A-H and J) were collected according to the survey design (Figure 1) but note that data from block D were not included in this exercise.

```{r montrave, echo=FALSE, fig.cap="Location of sika deer survey in southern Scotland and the survey design (from Marques *et al.* (2001). Note the differing amounts of effort in different woodlands based on information derived from pilot surveys.", out.width = '50%'}
knitr::include_graphics("https://workshops.distancesampling.org/standrews-2019/intro/practicals/figures/Prac_9_Figure_1.png")
```

In addition to these data, we also require estimates of the production rate. From a literature search, we learn that sika deer produce 25 pellet groups daily but this source did not provide a measure of variability of this estimate. During the course of our surveys we also followed the fate of some marked pellet groups to estimate the decay (disappearance) rates of a pellet group. A thorough discussion of methods useful for estimating decay rates and associated measures of precision can be found in Laing *et al.* [-@Laietal03].

There are many factors that might influence both production and decay rates, and for purposes of this exercise we will make the simplifying assumption that decay rate is homogeneous across these woodlands; with their mean time to decay of 163 days and a standard error of 13 days. (If you were to conduct a survey such as this, you would want to investigate this assumption more thoroughly.)

## Getting started

These data (called `sikadeer`) are available in the `Distance` package. As in previous exercises the conversion units are calculated. What are the measurement units for these data?

```{r, echo=T, eval=F}
library(Distance)
data(sikadeer)
# Work out conversion units
conversion.factor <- convert_units("centimeter", "kilometer", "square kilometer")
```

## Fit detection function to dung pellets

Fit the usual series of models (i.e. half normal, hazard rate, uniform) models to the distances to pellet groups and decide on a detection function (don't spend too long on this). Call your model `deer.df`. This detection function will be used to obtain $\hat D_{\textrm{pellet groups}}$.

Have a look at the `Summary statistics` for this model - what do you notice about the allocation of search effort in each woodland?

## Multipliers

The next step is to create an object which contains the multipliers we wish to use. We already have estimates of dung production rates but need similar information on dung decay (or persistence) rate.

Data to calculate this has been collected in the file `IntroDS_9.1.csv` in your directory. Following code comes from Meredith [-@Meredith2017].

```{r, eval=F}
MIKE.persistence <- function(DATA) {
  
#  Purpose: calculate mean persistence time (mean time to decay) for dung/nest data 
#  Input: data frame with at least two columns:
#         DAYS - calendar day on which dung status was observed
#         STATE - dung status: 1-intact, 0-decayed
#  Output: point estimate, standard error and CV of mean persistence time
#
#  Attribution: code from Mike Meredith website: 
#      http://www.mikemeredith.net/blog/2017/Sign_persistence.htm
#   Citing: CITES elephant protocol
#      https://cites.org/sites/default/files/common/prog/mike/survey/dung_standards.pdf
  
  ##   Fit logistic regression model to STATE on DAYS, extract coefficients
  dung.glm <- glm(STATE ~ DAYS, data=DATA, family=binomial(link = "logit"))
  betas <- coefficients(dung.glm)
  ##   Calculate mean persistence time
  mean.decay <- -(1+exp(-betas[1])) * log(1+exp(betas[1])) / betas[2]
  ## Calculate the variance of the estimate
  vcovar <- vcov(dung.glm)
  var0 <- vcovar[1,1]  # variance of beta0
  var1 <- vcovar[2,2]  # variance of beta1
  covar <- vcovar[2,1] # covariance
  deriv0 <- -(1-exp(-betas[1]) * log(1+exp(betas[1])))/betas[2]
  deriv1 <- -mean.decay/betas[2]
  var.mean <- var0*deriv0^2 + 2*covar*deriv0*deriv1 + var1*deriv1^2
  ## Calculate the SE and CV and return
  se.mean <- sqrt(var.mean)
  cv.mean <- se.mean/mean.decay
  out <- c(mean.decay, se.mean, 100*cv.mean)
  names(out) <- c("Mean persistence time", "SE", "%CV")
  plot(decay$DAYS, jitter(decay$STATE, amount=0.10), xlab="Days since initiation",
       ylab="Dung persists (yes=1)",
       main="Eight dung piles revisited over time")
  curve(predict(dung.glm, data.frame(DAYS=x), type="resp"), add=TRUE)
  abline(v=mean.decay, lwd=2, lty=3)
  return(out)
}
decay <- read.csv("IntroDS_9.1.csv")
persistence.time <- MIKE.persistence(decay)
print(persistence.time)
```

Running the above command should have produced a plot of dung persistence versus days since produced and fitted a logistic regression (this is like a simple linear regression but restricts the response to taking values between 0 and 1). Note the points can in reality only take values between 0 and 1 but for the purposes of plotting have been 'jittered' to avoid over-plotting.

An estimate of mean persistence time and measure of variability are also provided - make a note of these as they will be required below.

As stated above, we want an object which contains information on the dung production rate (and standard error) and dung decay rate (and standard error). The following command creates a list containing two data frames:

-   `creation` contains estimates of the dung production rate and associated standard error
-   `decay` contains the dung decay rate and associated standard error where `XX` and `YY` are the estimates you obtained from the dung decay rate analysis.

```{r, echo=T, eval=F}
# Create list of multipliers
mult <- list(creation = data.frame(rate=25, SE=0),
#             decay    = data.frame(rate=XX, SE=YY))
print(mult)
```

The final step is to use these multipliers to convert $\hat D_{\textrm{pellet groups}}$ to $\hat D_{\textrm{deer}}$ (as in the equations above) - for this we need to employ the `dht2` function. In the command below the `multipliers=` argument allows us to specify the rates and standard errors. There are a couple of other function arguments that need some explanation:

-   `strat_formula=~Region.Label` is specified to take into account the design (i.e. different woodlands or blocks).
-   `stratification="effort_sum"` is specified because we want to produce an overall estimate density that is the mean of the woodland specific densities weighted by effort allocated within each block.
-   `deer.df` is the detection function you have fitted.

```{r, echo=T, eval=F}
# Weight by effort because we have repeats
deer.ests <- dht2(deer.df, flatfile=sikadeer, strat_formula=~Region.Label,
                 convert_units=conversion.factor, multipliers=mult, 
                 stratification="effort_sum", total_area=13.9)
print(deer.ests)
```

The function `dht2` also provides information on the components of variance. Make a note of the these (contribution of detection function, encounter rate, decay rate and what happened to production rate component?) in each strata.

# Cue counting survey of whales {#cue-counting-survey-of-whales}

This exercise involves analysing an aerial cue counting survey of whales in the Atlantic and the species in this exercise tend to occur singly. An estimate of mean cue rate and its coefficient of variation have been obtained from tagging studies on a number of whales in the region.

The sample size is relatively small for a cue counting survey (which require larger sample sizes for reliable estimation of the detection function than line transect surveys), but this was the sample that was generated by the (expensive) survey, so carry on as usual. A stratified design was used (note different values of `Region.Label`), but we won't conduct a stratified analysis, so attention can be focused upon the cue counting aspect of this analysis.

The data are stored in the `Distance` package under `CueCountingExample`. In the command below, the data object is renamed to a shorter name. In these data, there is a column called search time - this information is copied to a column called `Effort` - this is needed by `ds`.

```{r, echo=T, eval=F}
data(CueCountingExample)
head(CueCountingExample, n=3)
# Create effort column
CueCountingExample$Effort <- CueCountingExample$Search.time
```

You can see that these data contain columns containing cue rate information - what is the cue rate and standard error? This information needs to be in a list format for the `dht2` function: this object can be created in a similar way to the `mult` object in problem 1 but here, we create this object from the survey data. Note, there is only one multiplier (for cue rate) required in this problem.

```{r, echo=T, eval=F}
# Obtain the cue rates from the survey data
# Select relevant columns
cuerates <- CueCountingExample[ ,c("Cue.rate", "Cue.rate.SE", "Cue.rate.df")]
# Only save unique values
cuerates <- unique(cuerates)
# Rename columns 
names(cuerates) <- c("rate", "SE", "df")
# Create multiplier object
mult <- list(creation=cuerates)
print(mult)
```

Decide on a truncation distance and fit a suitable detection function to these data: call your selected model `whale.df`. Remember that these data are treated as coming from a point transect.

Use the `dht2` function to estimate whale abundance in the survey region, together with a 95% confidence interval. As well as specifying the multipliers, the sampling fraction argument (`sample_fraction=`) also needs to be specified - in this case, half the circle was searched and so what do you think the sampling fraction should be? In the command below, density estimates are obtained unstratified (i.e. ignoring the survey regions).

```{r, echo=T, eval=F}
# Estimate density - what is the sampling fraction?
whale.est <- dht2(whale.df, flatfile=CueCountingExample, strat_formula=~1,
                  multipliers=mult, sample_fraction=?)
print(whale.est)
```

# Cue counting of songbirds optional {#cue-counting-of-songbirds-optional}

Remember the wren data that was introduced in the point transect exercises (Exercise 5): another data collection method that was used was cue counting. In this case, the cue was a song burst. These data are stored in `wren_cuecount` in the `Distance` package. The data arose from the point transect survey of songbirds described in Buckland [-@buckland2006].

Following a similar approach to that of the whale data, estimate a detection function of songs, create a multipliers object and include this in the `dht2` function to estimate wren density. Call this `w3.est`). There are a few things that will be useful to know:

-   search effort (measured in time) was 2 visits each lasting 5 minutes,
-   effort will need to be properly specified before fitting the detection function with `ds`
-   multiplier in this case is cue rate and its measure of precision,
-   the multiplier needs to be created before making the call to `dht2`

What do you think the sampling fraction will be for these point transects?

```{r, eval=FALSE, echo=TRUE}
data("wren_cuecount")
conversion.factor <- convert_units("meter", NULL, "hectare")
w3.est <- dht2(w3.hr, flatfile=wren_cuecount, strat_formula=~1,
               multipliers=mult, convert_units=conversion.factor)
```

To obtain density (rather than abundance) use the following command:

```{r, eval=F}
print(w3.est, report="density")
```

# References
